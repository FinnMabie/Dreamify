{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#This code has been adapted from: https://www.tensorflow.org/tutorials/generative/dcgan which creates artificial 28x28\n#grayscale handwritten digits.\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport os\nimport PIL\nimport pathlib\nimport tensorflow\ntf.config.run_functions_eagerly(True)\nfrom IPython import display\nfrom PIL import Image\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.python.ops.numpy_ops import np_config\nimport glob\n\ndata_dir = \"../input/simpsons-faces/cropped\"\ndata_dir =pathlib.Path(data_dir)\nimg_height = 120\nimg_width = 120\n\nimages = glob.glob(str(data_dir)+\"/*.png\") #Retrieves images from data directory\nimages_size = len(images) #Needed later to calculate the duration of one epoch\nimages_size","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-01T15:36:45.342988Z","iopub.execute_input":"2022-05-01T15:36:45.343251Z","iopub.status.idle":"2022-05-01T15:36:45.386108Z","shell.execute_reply.started":"2022-05-01T15:36:45.343224Z","shell.execute_reply":"2022-05-01T15:36:45.385342Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"import glob\nimport cv2\nfrom random import shuffle\n\n#Generator for loading images\n#We used a data generator to preserve memory when this program is run\n#We Converted the images from RGB color space to LAB so that the generator model only needs to fit on two dimensions(A,B)\n#Whereas RGB would have requiered fitting on all 3. This improved performance and efficiency.\n#The L in LAB represents the lightness of an image so it is practically just a grayscale image which preserves the \n#images structure allowing the generator model to have a much easier time producing images\ndef load_images(path, size=(120,120), batch = 64):    \n    images = glob.glob(str(path)+\"/*.png\")\n    shuffle(images)\n    for image in images:\n        img_list = []\n        gray_list = []\n        #These lists are effectively batches (Of size 64 in this case)\n        for i in range(batch):\n            loaded = cv2.imread(image)\n            #Resize images so they fit into the Generator and Discriminator models\n            loaded = cv2.resize(loaded, size, interpolation = cv2.INTER_AREA)\n            img_list.append(cv2.cvtColor(loaded, cv2.COLOR_BGR2LAB)) #Default for cv2 is BGR so need to convert to LAB\n            gray_list.append(cv2.cvtColor(loaded, cv2.COLOR_BGR2GRAY)) #Batch for grayscale images\n        yield np.asarray(img_list)/255, np.expand_dims(np.asarray(gray_list),axis=3)/255\n        #yield both batches and use expand dims so that grayscale output has dimensions 120,120,1 (Needed to fit the models)\n        #Yield allows us to preserve memory","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:45.419360Z","iopub.execute_input":"2022-05-01T15:36:45.419590Z","iopub.status.idle":"2022-05-01T15:36:45.428233Z","shell.execute_reply.started":"2022-05-01T15:36:45.419565Z","shell.execute_reply":"2022-05-01T15:36:45.427539Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"#Code to show LAB image and Grayscale image\n#LAB can easily be converted to RGB so we will do this after the model trains on LAB images\nimport matplotlib.pyplot as plt\ntest = load_images(data_dir)\nimg, gray = next(test)\nplt.imshow(img[0])\nplt.show()\nplt.imshow(gray[0], cmap=\"gray\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:45.495253Z","iopub.execute_input":"2022-05-01T15:36:45.495922Z","iopub.status.idle":"2022-05-01T15:36:46.069033Z","shell.execute_reply.started":"2022-05-01T15:36:45.495887Z","shell.execute_reply":"2022-05-01T15:36:46.068340Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#Generator model borrowed from: https://github.com/OvaizAli/Image-Colorization-using-GANs\n#Input: Grayscale image (120, 120, 1)\n#Output: LAB image (120, 120, 3)\ndef make_generator_model():\n    inputs = tf.keras.layers.Input( shape=( 120 , 120 , 1 ) )\n    \n    conv1 = tf.keras.layers.Conv2D( 16 , kernel_size=( 5 , 5 ) , strides=1)( inputs )\n    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )\n    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n    conv1 = tf.keras.layers.Conv2D( 32 , kernel_size=( 3 , 3 ) , strides=1)( conv1 )\n    conv1 = tf.keras.layers.LeakyReLU()( conv1 )\n\n    conv2 = tf.keras.layers.Conv2D( 32 , kernel_size=( 5 , 5 ) , strides=1)( conv1 )\n    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1)( conv2 )\n    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n    conv2 = tf.keras.layers.Conv2D( 64 , kernel_size=( 3 , 3 ) , strides=1)( conv2 )\n    conv2 = tf.keras.layers.LeakyReLU()( conv2 )\n\n    conv3 = tf.keras.layers.Conv2D( 64 , kernel_size=( 5 , 5 ) , strides=1)( conv2 )\n    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1)( conv3 )\n    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n    conv3 = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1)( conv3 )\n    conv3 = tf.keras.layers.LeakyReLU()( conv3 )\n\n    bottleneck = tf.keras.layers.Conv2D( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='tanh' , padding='same' )( conv3 )\n\n    concat_1 = tf.keras.layers.Concatenate()( [ bottleneck , conv3 ] )\n    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_1 )\n    conv_up_3 = tf.keras.layers.Conv2DTranspose( 128 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_3 )\n    conv_up_3 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_3 )\n\n    concat_2 = tf.keras.layers.Concatenate()( [ conv_up_3 , conv2 ] )\n    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( concat_2 )\n    conv_up_2 = tf.keras.layers.Conv2DTranspose( 64 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu' )( conv_up_2 )\n    conv_up_2 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' )( conv_up_2 )\n\n    concat_3 = tf.keras.layers.Concatenate()( [ conv_up_2 , conv1 ] )\n    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( concat_3 )\n    conv_up_1 = tf.keras.layers.Conv2DTranspose( 32 , kernel_size=( 3 , 3 ) , strides=1 , activation='relu')( conv_up_1 )\n    conv_up_1 = tf.keras.layers.Conv2DTranspose( 3 , kernel_size=( 5 , 5 ) , strides=1 , activation='sigmoid')( conv_up_1 )\n    model = tf.keras.models.Model( inputs , conv_up_1 )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.070852Z","iopub.execute_input":"2022-05-01T15:36:46.071332Z","iopub.status.idle":"2022-05-01T15:36:46.090608Z","shell.execute_reply.started":"2022-05-01T15:36:46.071292Z","shell.execute_reply":"2022-05-01T15:36:46.089834Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"#Create generator model\nmake_generator_model()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.091822Z","iopub.execute_input":"2022-05-01T15:36:46.092092Z","iopub.status.idle":"2022-05-01T15:36:46.294314Z","shell.execute_reply.started":"2022-05-01T15:36:46.092056Z","shell.execute_reply":"2022-05-01T15:36:46.293560Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"#See what untrained generator produces when fed a random grayscale image\n#Mostly gray because sigmoid function returns numbers in range (0,1) so many numbers lie around 0.5.\ngenerator = make_generator_model()\ngenerated_image = generator(np.expand_dims(gray[0], axis=0), training=False) #need expand dims to have batch size\nplt.imshow(generated_image[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.296700Z","iopub.execute_input":"2022-05-01T15:36:46.296978Z","iopub.status.idle":"2022-05-01T15:36:46.694056Z","shell.execute_reply.started":"2022-05-01T15:36:46.296933Z","shell.execute_reply":"2022-05-01T15:36:46.693344Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"#GDiscriminator model borrowed from: https://github.com/OvaizAli/Image-Colorization-using-GANs\n#Input: LAB image (120, 120, 3)\n#Output: A number (positive or negative)\n#The model will be trained to output positive values for real colorized images, and negative values for fake colorized images.\nfrom keras.models import Model\nfrom keras.layers import Conv2D, BatchNormalization, Activation, Dropout, Flatten, Dense, Input, LeakyReLU, Conv2DTranspose,AveragePooling2D\ndef make_discriminator_model():\n    model = Sequential()\n    model.add(Conv2D(32,(3,3), padding='same',strides=2,input_shape=(120,120,3)))\n    model.add(LeakyReLU(0.2))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64,(3,3),padding='same',strides=2))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(.2))\n    model.add(Dropout(0.25))\n\n\n    model.add(Conv2D(128,(3,3), padding='same', strides=2))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(0.2))\n    model.add(Dropout(0.25))\n\n\n    model.add(Conv2D(256,(3,3), padding='same',strides=2))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(0.2))\n    model.add(Dropout(0.25))\n\n\n    model.add(Flatten())\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    image = Input(shape=(120,120,3))\n    validity = model(image)\n    return Model(image,validity)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.695214Z","iopub.execute_input":"2022-05-01T15:36:46.695455Z","iopub.status.idle":"2022-05-01T15:36:46.707248Z","shell.execute_reply.started":"2022-05-01T15:36:46.695421Z","shell.execute_reply":"2022-05-01T15:36:46.706401Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#See what happens when untrained discriminator is fed a generated \"fake\" colorized image\ndiscriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint(decision)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.709240Z","iopub.execute_input":"2022-05-01T15:36:46.710118Z","iopub.status.idle":"2022-05-01T15:36:46.842937Z","shell.execute_reply.started":"2022-05-01T15:36:46.710045Z","shell.execute_reply":"2022-05-01T15:36:46.842157Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.844258Z","iopub.execute_input":"2022-05-01T15:36:46.844515Z","iopub.status.idle":"2022-05-01T15:36:46.848102Z","shell.execute_reply.started":"2022-05-01T15:36:46.844470Z","shell.execute_reply":"2022-05-01T15:36:46.847418Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#Loss function for the discriminator model\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output) - tf.random.uniform( shape=real_output.shape , maxval=0.1 ) , real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output) + tf.random.uniform( shape=fake_output.shape , maxval=0.1  ) , fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.849473Z","iopub.execute_input":"2022-05-01T15:36:46.849960Z","iopub.status.idle":"2022-05-01T15:36:46.858474Z","shell.execute_reply.started":"2022-05-01T15:36:46.849925Z","shell.execute_reply":"2022-05-01T15:36:46.857741Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"#Loss function for the generator model\nmse = tf.keras.losses.MeanSquaredError()\ndef generator_loss(fake_output , real_y):\n    real_y = tf.cast( real_y , 'float32' )\n    return mse( fake_output , real_y )","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.861368Z","iopub.execute_input":"2022-05-01T15:36:46.861629Z","iopub.status.idle":"2022-05-01T15:36:46.870323Z","shell.execute_reply.started":"2022-05-01T15:36:46.861605Z","shell.execute_reply":"2022-05-01T15:36:46.869623Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"#Both models use the ADAM optimization algorithm\ngenerator_optimizer = tf.keras.optimizers.Adam(0.0001)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(0.0001)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.874414Z","iopub.execute_input":"2022-05-01T15:36:46.875640Z","iopub.status.idle":"2022-05-01T15:36:46.880833Z","shell.execute_reply.started":"2022-05-01T15:36:46.875610Z","shell.execute_reply":"2022-05-01T15:36:46.880046Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"#Used to save and restore models, which can be helpful in case a long running training task is interrupted.\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.883520Z","iopub.execute_input":"2022-05-01T15:36:46.884107Z","iopub.status.idle":"2022-05-01T15:36:46.896640Z","shell.execute_reply.started":"2022-05-01T15:36:46.884068Z","shell.execute_reply":"2022-05-01T15:36:46.895686Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"#Initialize variables used later on\nEPOCHS = 150\nBATCH_SIZE = 64","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.899597Z","iopub.execute_input":"2022-05-01T15:36:46.899922Z","iopub.status.idle":"2022-05-01T15:36:46.902913Z","shell.execute_reply.started":"2022-05-01T15:36:46.899893Z","shell.execute_reply":"2022-05-01T15:36:46.902250Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"gen_loss_list = []\ndisc_loss_list = []\n#Used to plot loss later on","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.904423Z","iopub.execute_input":"2022-05-01T15:36:46.904974Z","iopub.status.idle":"2022-05-01T15:36:46.911530Z","shell.execute_reply.started":"2022-05-01T15:36:46.904938Z","shell.execute_reply":"2022-05-01T15:36:46.910791Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(input_x, real_y):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(input_x, training=True)\n        real_output = discriminator(real_y, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(generated_images, real_y)\n        disc_loss = discriminator_loss(real_output, fake_output)\n        gen_loss_list.append(gen_loss.numpy())\n        disc_loss_list.append(disc_loss.numpy())\n    \n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\ngenerator.compile(\n    optimizer=generator_optimizer,\n    loss=generator_loss,\n    metrics=['accuracy']\n)\n\ndiscriminator.compile(\n    optimizer=discriminator_optimizer,\n    loss=discriminator_loss,\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.913548Z","iopub.execute_input":"2022-05-01T15:36:46.914052Z","iopub.status.idle":"2022-05-01T15:36:46.932887Z","shell.execute_reply.started":"2022-05-01T15:36:46.914016Z","shell.execute_reply":"2022-05-01T15:36:46.932139Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"#using steps instead of epochs because generator does not know when it has traversed one batch\n#So instead we calculate epoch number ourselves and break the loop when the correct number of epochs has been reached\n#1 epoch = images_size/BATCH_SIZE\ndef train(dataset, epochs):\n    total_steps = 0 #Used to track how many times the train_step function has been called\n    epoch = 0\n    start = time.time() #Used to print how long each epoch takes to run\n    for (y, x) in dataset:\n        total_steps += 1\n        if total_steps % (images_size//BATCH_SIZE) == 0: #If one epoch has passed\n            display.clear_output(wait=True) #Clear window\n            epoch+=1\n            generate_and_save_images(generator, epoch + 1, gray[0:1]) #Save images for GIF later\n            print ('Time for epoch {} is {} sec'.format(epoch, time.time()-start))\n            start = time.time() #Used to print how long each epoch takes to run \n        if epoch >= epochs: #Break loop when total number of epochs have executed\n            break\n        train_step(x,y)\n    \n    # Generate Image after the final epoch\n    display.clear_output(wait=True)\n    generate_and_save_images(generator, epoch + 1, gray[0:1])#change to numpy to dim...\n    print ('Time for epoch {} is {} sec'.format(epoch, time.time()-start))\n    display.clear_output(wait=True)\n    generate_and_save_images(generator, epoch, gray[0:1])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.934234Z","iopub.execute_input":"2022-05-01T15:36:46.934486Z","iopub.status.idle":"2022-05-01T15:36:46.944432Z","shell.execute_reply.started":"2022-05-01T15:36:46.934452Z","shell.execute_reply":"2022-05-01T15:36:46.943653Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n    print(\"Image Generated\")\n    predictions = model(test_input, training=False) #change later to np to dim...\n    prediction = predictions[0]*255 #bring to 0-255 range (Was in [0,1] range because sigmoid was the activation function)\n    prediction = np.uint8(prediction)\n    prediction = cv2.cvtColor(prediction, cv2.COLOR_LAB2RGB)\n    prediction = prediction #normalize the image\n    plt.imshow(prediction)\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()\n    print(\"Real\")\n    actual = np.uint8(img[0]*255)\n    actual = cv2.cvtColor(actual, cv2.COLOR_LAB2RGB)\n    plt.imshow(actual)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.945764Z","iopub.execute_input":"2022-05-01T15:36:46.946214Z","iopub.status.idle":"2022-05-01T15:36:46.956163Z","shell.execute_reply.started":"2022-05-01T15:36:46.946161Z","shell.execute_reply":"2022-05-01T15:36:46.955360Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"train(load_images(data_dir), EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:36:46.957703Z","iopub.execute_input":"2022-05-01T15:36:46.957956Z","iopub.status.idle":"2022-05-01T15:45:48.292695Z","shell.execute_reply.started":"2022-05-01T15:36:46.957924Z","shell.execute_reply":"2022-05-01T15:45:48.291345Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"#Plot both loss functions over a period of train steps (Using smoothing)\nfrom scipy.interpolate import make_interp_spline\nx = list(range(0, len(gen_loss_list)))\ny = gen_loss_list\nX_Y_Spline = make_interp_spline(x, y)\nX_ = np.linspace(np.min(x), np.max(x), 300)\nY_ = X_Y_Spline(X_)\nplt.title('Generator Loss')\nplt.xlabel('Step Number')\nplt.ylabel('Loss')\nplt.plot(X_, Y_)\nplt.show()\n\nx = list(range(0, len(disc_loss_list)))\ny = disc_loss_list\nX_Y_Spline = make_interp_spline(x, y)\nX_ = np.linspace(np.min(x), np.max(x), 500)\nY_ = X_Y_Spline(X_)\nplt.title('Discriminator Loss')\nplt.xlabel('Step Number')\nplt.ylabel('Loss')\nplt.plot(X_, Y_)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:45:51.697608Z","iopub.execute_input":"2022-05-01T15:45:51.697888Z","iopub.status.idle":"2022-05-01T15:45:52.066851Z","shell.execute_reply.started":"2022-05-01T15:45:51.697859Z","shell.execute_reply":"2022-05-01T15:45:52.066157Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"#Plot exmaple grayscale, colorized, and real version of exmaple character on test dataset\ndata_dir = \"../input/simpsons-vs-real-faces/simpsons_vs_real_dataset/testB\"\ntest = load_images(data_dir)\nimg, gray = next(test)\nprint(\"Grayscale\")\nplt.imshow(gray[0], cmap=\"gray\")\nplt.show()\ngenerate_and_save_images(generator, EPOCHS, gray[0:1])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:45:54.157031Z","iopub.execute_input":"2022-05-01T15:45:54.157758Z","iopub.status.idle":"2022-05-01T15:45:54.932698Z","shell.execute_reply.started":"2022-05-01T15:45:54.157722Z","shell.execute_reply":"2022-05-01T15:45:54.932012Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# Display a single image using the epoch number\ndef display_image(epoch_no):\n    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no-1))\ndisplay_image(EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:45:58.346053Z","iopub.execute_input":"2022-05-01T15:45:58.346682Z","iopub.status.idle":"2022-05-01T15:45:58.383295Z","shell.execute_reply.started":"2022-05-01T15:45:58.346631Z","shell.execute_reply":"2022-05-01T15:45:58.382266Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"#used to create a GIF of the learning process through epochs\nimport imageio\nimport glob\nanim_file = 'colorize.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n    filenames = glob.glob('image*.png')\n    filenames = sorted(filenames)\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:46:00.835131Z","iopub.execute_input":"2022-05-01T15:46:00.835383Z","iopub.status.idle":"2022-05-01T15:46:01.150799Z","shell.execute_reply.started":"2022-05-01T15:46:00.835354Z","shell.execute_reply":"2022-05-01T15:46:01.150084Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"#Need to run: pip install git+https://github.com/tensorflow/docs\nimport tensorflow_docs.vis.embed as embed\nembed.embed_file(anim_file)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T15:46:03.066951Z","iopub.execute_input":"2022-05-01T15:46:03.067803Z","iopub.status.idle":"2022-05-01T15:46:03.089432Z","shell.execute_reply.started":"2022-05-01T15:46:03.067758Z","shell.execute_reply":"2022-05-01T15:46:03.088392Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}